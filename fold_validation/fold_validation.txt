There are many different ways one can do cross-validation, and it is the most critical
step when it comes to building a good machine learning model which is
generalizable when it comes to unseen data. Choosing the right cross-validation
depends on the dataset you are dealing with, and one’s choice of cross-validation
on one dataset may or may not apply to other datasets. However, there are a few
types of cross-validation techniques which are the most popular and widely used.

These include:
• k-fold cross-validation
• stratified k-fold cross-validation
• hold-out based validation
• leave-one-out cross-validation
• group k-fold cross-validation

• k-fold cross-validation
	We can use this process with almost all kinds of datasets. 
For example, when you have images, you can create a CSV with 
image id, image location and image label and use the process above.
simple k-fold cross-validation works for any regression problem
However, if you see that the distribution of targets is not
consistent, you can use stratified k-fold.

• stratified k-fold cross-validation
	If you have a skewed dataset for binary classification with 90% positive 
samples and only 10% negative samples Stratified k-fold cross-validation keeps 
the ratio of labels in each fold constant. So, in each fold, you will have the
same 90% positive and 10% negative samples. Thus, whatever metric you choose to
evaluate, it will give similar results across all folds.
	Some classeshave a lot of samples, and some don’t have that many. If we 
do a simple k-fold, we won’t have an equal distribution of targets in every fold. 
Thus, we choose stratified k-fold in this case.
The rule is simple. If it’s a standard classification problem, choose stratified k-fold
blindly.

• hold-out based validation
	Suppose we have 1 million samples. A 5 fold cross-validation would mean 
training on 800k samples and validating on 200k. Depending on which algorithm we
choose, training and even validation can be very expensive for a dataset which is 
of this size. In these cases, we can opt for a hold-out based validation.
	The process for creating the hold-out remains the same as stratified k-fold. For a
dataset which has 1 million samples, we can create ten folds instead of 5 and keep
one of those folds as hold-out. This means we will have 100k samples in the hold-
out, and we will always calculate loss, accuracy and other metrics on this set and
train on 900k samples.
	Hold-out is also used very frequently with time-series data. for Example 
Let’s assume the problem we are provided with is predicting sales of a store for 2020, and you are
provided all the data from 2015-2019. In this case, you can select all the data for
2019 as a hold-out and train your model on all the data from 2015 to 2018.


In many cases, we have to deal with small datasets and creating big validation sets
means losing a lot of data for the model to learn. In those cases, we can opt for a
type of k-fold cross-validation where k=N, where N is the number of samples in the
dataset. This means that in all folds of training, we will be training on all data
samples except 1. The number of folds for this type of cross-validation is the same
as the number of samples that we have in the dataset.


***********Now we can move to regression. The good thing about regression problems is that
we can use all the cross-validation techniques mentioned above for regression problems except for stratified k-fold**************

